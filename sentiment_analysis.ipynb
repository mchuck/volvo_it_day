{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/maciek/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "seed =  42\n",
    "MAX_SENTENCES = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "0 @ home studying for maths wooot ! im so going to fail this shit \n",
      "0 Pickin up @misstinayao waitin on @sadittysash 2 hurry up...I odeeee missed dem  Table talk 2nite...LOL bout to be fat...\n",
      "0 @ProudGamerTweet I rather average 32370 \n",
      "1 @officialnjonas Good luck with that \n",
      "0 this song's middle change just doesn't want to be born..... arghhhh!! \n",
      "0 im starting my many hours of work now \n",
      "0 Thunderstorms yesterday, more on the way. Looks like I won't be online much again today.  HAPPY FATHER'S DAY\n",
      "1 @cloecouturier Yes, I do have a few ..   4 Girls ...   You son is 23 ... all grown up now .. happens fast!!\n",
      "0 Last free travel at AP-1  http://yfrog.com/0uvu5j\n",
      "1 TF2 has updated but its way to late to play.  looks like I'll be spying and sniping my heart out tomorrow \n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "dataset_x = []\n",
    "dataset_y = []\n",
    "\n",
    "labels = {0:0, 4:1}\n",
    "\n",
    "with open('training.1600000.processed.noemoticon.csv', 'r', encoding='ISO-8859-1') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "        dataset.append((labels[int(line[0])], line[5]))\n",
    "    rng = np.random.RandomState(seed)\n",
    "    ids = rng.choice(len(dataset), MAX_SENTENCES)\n",
    "    smaller_dataset = [dataset[i] for i in ids]\n",
    "    dataset_x = [d[1] for d in smaller_dataset]\n",
    "    dataset_y = [d[0] for d in smaller_dataset]\n",
    "\n",
    "print(len(dataset_x))\n",
    "        \n",
    "def print_n(data_x, data_y, n=10):\n",
    "    for x,y in zip(data_x[:n], data_y[:n]):\n",
    "        print(y,x)\n",
    "        \n",
    "print_n(dataset_x, dataset_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 home studying for maths wooot ! im so going to fail this shit\n",
      "0 pickin up misstinayao waitin on sadittysash hurry up...i odeeee missed dem table talk nite...lol bout to be fat...\n",
      "0 proudgamertweet i rather average\n",
      "1 officialnjonas good luck with that\n",
      "0 this song's middle change just doesn't want to be born..... arghhhh!!\n",
      "0 im starting my many hours of work now\n",
      "0 thunderstorms yesterday, more on the way. looks like i won't be online much again today. happy father's day\n",
      "1 cloecouturier yes, i do have a few .. girls ... you son is ... all grown up now .. happens fast!!\n",
      "0 last free travel at ap http yfrog.com uvu j\n",
      "1 tf has updated but its way to late to play. looks like i'll be spying and sniping my heart out tomorrow\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_regex(sentence):\n",
    "    sentence = re.sub('[^a-z\\',.?! ]', ' ', sentence.lower())\n",
    "    sentence = re.sub('\\s+', ' ', sentence)\n",
    "    return sentence.strip()\n",
    "    \n",
    "dataset_x_clear = [preprocess_regex(x) for x in dataset_x]\n",
    "\n",
    "print_n(dataset_x_clear, dataset_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['home', 'studi', 'for', 'math', 'wooot', 'go', 'fail', 'thi', 'shit']\n",
      "0 ['pickin', 'misstinayao', 'waitin', 'sadittysash', 'hurri', '...', 'odeee', 'miss', 'dem', 'tabl', 'talk', 'nite', '...', 'lol', 'bout', 'fat', '...']\n",
      "0 ['proudgamertweet', 'rather', 'averag']\n",
      "1 ['officialnjona', 'good', 'luck', 'with', 'that']\n",
      "0 ['thi', \"song'\", 'middl', 'chang', 'just', \"doesn't\", 'want', 'born', '...', 'arghhhh']\n",
      "0 ['start', 'mani', 'hour', 'work', 'now']\n",
      "0 ['thunderstorm', 'yesterday', 'more', 'the', 'way', 'look', 'like', \"won't\", 'onlin', 'much', 'again', 'today', 'happi', \"father'\", 'day']\n",
      "1 ['cloecouturi', 'ye', 'have', 'few', 'girl', '...', 'you', 'son', '...', 'all', 'grown', 'now', 'happen', 'fast']\n",
      "0 ['last', 'free', 'travel', 'http', 'yfrog.com', 'uvu']\n",
      "1 ['ha', 'updat', 'but', 'it', 'way', 'late', 'play', 'look', 'like', \"i'll\", 'spi', 'and', 'snipe', 'heart', 'out', 'tomorrow']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "tokenizer = TweetTokenizer().tokenize\n",
    "stemmer = PorterStemmer().stem\n",
    "\n",
    "def tokenize_sentence(sentence):\n",
    "    sentence = tokenizer(sentence)\n",
    "    sentence = [stemmer(word) for word in sentence if len(word) > 2]\n",
    "    return sentence\n",
    "\n",
    "dataset_x_tokenized = [tokenize_sentence(x) for x in dataset_x_clear]\n",
    "\n",
    "print_n(dataset_x_tokenized, dataset_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 home studi math wooot go fail thi shit\n",
      "0 pickin misstinayao waitin sadittysash hurri ... odeee miss dem tabl talk nite ... lol bout fat ...\n",
      "0 proudgamertweet rather averag\n",
      "1 officialnjona good luck\n",
      "0 thi song' middl chang want born ... arghhhh\n",
      "0 start mani hour work\n",
      "0 thunderstorm yesterday way look like onlin much today happi father' day\n",
      "1 cloecouturi ye girl ... son ... grown happen fast\n",
      "0 last free travel http yfrog.com uvu\n",
      "1 ha updat way late play look like i'll spi snipe heart tomorrow\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(sentence):\n",
    "    sentence = [word for word in sentence if word not in stops]\n",
    "    return ' '.join(sentence)\n",
    "\n",
    "dataset_x_no_stopwords = [remove_stopwords(x) for x in dataset_x_tokenized]\n",
    "\n",
    "print_n(dataset_x_no_stopwords, dataset_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,3), max_features=5000)\n",
    "vectorizer.fit(dataset_x_no_stopwords)\n",
    "\n",
    "dataset_x_bow = vectorizer.transform(dataset_x_no_stopwords)\n",
    "\n",
    "print(dataset_x_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(dataset_x_bow)\n",
    "dataset_x_tfidf = tf_transformer.transform(dataset_x_bow)\n",
    "\n",
    "print(dataset_x_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=seed,solver='lbfgs',\n",
    "                          multi_class='multinomial').fit(dataset_x_tfidf, dataset_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(359, 5000)\n"
     ]
    }
   ],
   "source": [
    "test_x = []\n",
    "test_y = []\n",
    "\n",
    "with open('testdata.manual.2009.06.14.csv', 'r', encoding='ISO-8859-1') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "        if int(line[0]) != 2:\n",
    "            test_y.append(labels[int(line[0])])\n",
    "            test_x.append(line[5])\n",
    "        \n",
    "def process_sentences(sentences):\n",
    "    sentences = [preprocess_regex(s) for s in sentences]\n",
    "    sentences = [tokenize_sentence(s) for s in sentences]\n",
    "    sentences = [remove_stopwords(s) for s in sentences]\n",
    "    sentences = vectorizer.transform(sentences)\n",
    "    sentences = tf_transformer.transform(sentences)\n",
    "    return sentences\n",
    "\n",
    "test_x_processed = process_sentences(test_x)\n",
    "\n",
    "print(test_x_processed.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7771587743732591\n"
     ]
    }
   ],
   "source": [
    "score = clf.score(test_x_processed, test_y)\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive , [0.24013723 0.75986277] , Volvo IT days is an awesome conference!\n",
      "not sure , [0.46271552 0.53728448] , I spoke at Volvo IT days\n",
      "negative , [0.70568241 0.29431759] , It's a shame that my speach took only 30 mins!\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    'Volvo IT days is an awesome conference!',\n",
    "    'I spoke at Volvo IT days',\n",
    "    \"It's a shame that my speach took only 30 mins!\"\n",
    "]\n",
    "\n",
    "def get_label(proba, treshold=0.1):\n",
    "    if abs(proba[0]-proba[1]) < treshold:\n",
    "        return 'not sure'\n",
    "    else:\n",
    "        return 'positive' if proba[0] < proba[1] else 'negative'\n",
    "\n",
    "sentences_processed = process_sentences(sentences)\n",
    "\n",
    "probas = clf.predict_proba(sentences_processed)\n",
    "\n",
    "for sent, proba in zip(sentences, probas):\n",
    "    print(get_label(proba), ',', proba, ',', sent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
